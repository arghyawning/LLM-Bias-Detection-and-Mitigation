# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_yIHicpOyJK769tOmh5uVRkcgO-GYf4p
"""

# !pip install datasets

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset, Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
import numpy as np
from sklearn.metrics import accuracy_score, classification_report
import torch
# from transformers import AdamW, get_scheduler

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print("Using device:", device)

# Load and preprocess
file = "data_public.csv"
# df = pd.read_csv(file, on_bad_lines='skip')
print(df.columns)

label_map = {"From the Left": 0, "From the Center": 1, "From the Right": 2}
df = df.dropna(subset=["original_body", "bias"])
# print(df["bias"][0])
df["bias"] = df["bias"].map(label_map)
# df["article"] = df[""].astype(str)

# print(df["bias"].value_counts())

"""3 epochs
Stratified equally
AdamW
"""

# Create a 80/10/10 split for train/validation/test
train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df["bias"], random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df["bias"], random_state=42)

# Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)
test_dataset = Dataset.from_pandas(test_df)

import os
os.environ["WANDB_DISABLED"] = "true"

# bert model
# 768
# linear classifier: (B, S, 768) -> (B, 3)

# Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# def tokenize_function(examples):
#     return tokenizer(examples["original_body"], padding="max_length", truncation=True)

# # If your dataset already has a "label" column
# tokenized_dataset = dataset.map(tokenize_function, batched=True)

# If your dataset doesn't have a properly formatted "label" column
def preprocess_function(examples):
    # Tokenize the texts
    result = tokenizer(examples["original_body"], padding="max_length", truncation=True)
    # Map labels to ids
    result["labels"] = examples["bias"]
    return result

# tokenized_dataset = dataset.map(preprocess_function, batched=True)


# def tokenize(batch):
#     return tokenizer(batch["original_body"], padding=True, truncation=True)

# Then tokenize all three sets
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)
tokenized_test = test_dataset.map(preprocess_function, batched=True)

# Model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)
model.to(device)  # Explicitly push model to GPU

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc}

# optimizer = AdamW(
#     model.parameters(),
#     lr=2e-5,           # Learning rate - slightly lower than default for more stable training
#     betas=(0.9, 0.999),# Default momentum parameters
#     eps=1e-8,          # Small epsilon for numerical stability
#     weight_decay=0.01  # L2 regularization to prevent overfitting
# )


# Complete training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,  # Number of training epochs
    per_device_train_batch_size=16,  # Batch size for training
    per_device_eval_batch_size=16,  # Batch size for evaluation
    warmup_steps=500,  # Number of warmup steps for learning rate scheduler
    weight_decay=0.01,  # Strength of weight decay
    logging_dir="./logs",  # Directory for storing logs
    logging_steps=10,  # Log every X steps
    eval_strategy="epoch",
    save_strategy="epoch",  # Save checkpoint each epoch
    load_best_model_at_end=True,  # Load the best model when finished training
    report_to=None,  # Disable wandb and other reporting
    # use_wandb=False,
)

# # Learning rate scheduler (warm-up followed by linear decay)
# num_training_steps = len(tokenized_train) * training_args.num_train_epochs // training_args.per_device_train_batch_size
# scheduler = get_linear_schedule_with_warmup(
#     optimizer,
#     num_warmup_steps=0.1 * num_training_steps,  # 10% of steps for warm-up
#     num_training_steps=num_training_steps
# )

# Define the trainer with your training dataset
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,  # Add your training dataset here
    eval_dataset=tokenized_val,
    compute_metrics=compute_metrics,
    # optimizers=optimizer,
)

# Train the model
trainer.train()

eval_results_after = trainer.evaluate(tokenized_test)
print("\n\nAfter training:", eval_results_after)
print("\n\n")

preds = trainer.predict(tokenized_test)
y_true = preds.label_ids
y_pred = np.argmax(preds.predictions, axis=1)

print("\n\n")
print(classification_report(y_true, y_pred, target_names=["left", "center", "right"]))

