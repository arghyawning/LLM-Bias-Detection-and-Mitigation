# -*- coding: utf-8 -*-
"""GPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E_L9EmhftNjub2-_ovSqCJzAgJL4-vxf
"""

# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM

# !pip install accelerate -U

# from transformers import (
#     AutoTokenizer, AutoModelForCausalLM,
#     TextDataset, DataCollatorForLanguageModeling,
#     Trainer, TrainingArguments
# )

# # Load GPT-2
# tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")
# model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")

# # Load dataset
# dataset = TextDataset(
#     tokenizer=tokenizer,
#     file_path="politics.txt",
#     block_size=64
# )

# data_collator = DataCollatorForLanguageModeling(
#     tokenizer=tokenizer,
#     mlm=False
# )

# import accelerate

# !pip install "transformers[torch]"

# from transformers import Trainer, TrainingArguments

# # Training configuration
# training_args = TrainingArguments(
#     output_dir="./gpt2-politics",
#     overwrite_output_dir=True,
#     num_train_epochs=5,
#     per_device_train_batch_size=2,
#     save_steps=100,
#     save_total_limit=1,
#     logging_steps=10,
#     prediction_loss_only=True
# )

# # Train
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     data_collator=data_collator,
#     train_dataset=dataset
# )

# trainer.train()

# def predict_political_leaning(text_input):
#     model.eval()
#     prompt = f"text: {text_input} pol:"
#     inputs = tokenizer.encode(prompt, return_tensors="pt")
#     outputs = model.generate(
#         inputs,
#         max_length=inputs.shape[1] + 2,
#         pad_token_id=tokenizer.eos_token_id
#     )
#     result = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     return result.split("pol:")[-1].strip()

# # Test
# print(predict_political_leaning("Taxing the rich is necessary for equity."))
# print(predict_political_leaning("Private enterprise drives innovation."))
# print(predict_political_leaning("Let's hear both sides before deciding."))

# file = "data_public.csv"
# df = pd.read_csv(file, on_bad_lines='skip')
# print(df.columns)
# df = df.dropna(subset=["original_body", "bias"])

# # Create a 80/10/10 split for train/validation/test
# train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df["bias"], random_state=42)
# val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df["bias"], random_state=42)

# # Convert to HuggingFace Datasets
# train_dataset = Dataset.from_pandas(train_df)
# val_dataset = Dataset.from_pandas(val_df)
# test_dataset = Dataset.from_pandas(test_df)

# !pip install datasets

import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    DataCollatorForLanguageModeling, Trainer, TrainingArguments
)

# Load the CSV
df = pd.read_csv("data_public.csv", on_bad_lines='skip')  # Change to your actual filename
df = df.dropna(subset=["original_body", "bias"])

df['bias'] = df['bias'].replace({
    "From the Left": "left",
    "From the Right": "right",
    "From the Center": "center"
})

# Prepare prompt-completion format
df["text"] = df["original_body"].apply(lambda x: f"text: {x.strip()} pol:")
df["label"] = df["bias"].str.strip()

# Format the input for causal LM
df["input_text"] = df.apply(lambda row: row["text"] + " " + row["label"], axis=1)

# Train/val/test split
train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df["label"], random_state=42)

# Convert to Hugging Face datasets
train_dataset = Dataset.from_pandas(train_df[["input_text"]])
val_dataset = Dataset.from_pandas(val_df[["input_text"]])
test_dataset = Dataset.from_pandas(test_df[["input_text"]])

# Tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Tokenize datasets
def tokenize_function(example):
    return tokenizer(example["input_text"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Training args
training_args = TrainingArguments(
    output_dir="./gpt2-bias",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=10,
    weight_decay=0.01,
    push_to_hub=False,
    load_best_model_at_end=True,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

# Inference function
def predict_political_leaning(text_input):
    model.eval()
    prompt = f"text: {text_input.strip()} pol:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=inputs["input_ids"].shape[1] + 2)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded.split("pol:")[-1].strip()

# Example test
print(predict_political_leaning("Universal healthcare should be a right."))
print(predict_political_leaning("Market deregulation promotes efficiency."))

from tqdm import tqdm

def evaluate_on_test_set(test_df):
    model.eval()
    correct = 0
    total = 0

    for story, true_label in tqdm(zip(test_df["original_body"], test_df["bias"]), total=len(test_df)):
        predicted = predict_political_leaning(story)
        # prompt = f"text: {story.strip()} pol:"
        # inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        # with torch.no_grad():
        #     outputs = model.generate(**inputs, max_length=inputs["input_ids"].shape[1] + 2)
        # decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # predicted = decoded.split("pol:")[-1].strip().lower()
        # # Keep only one-word predictions
        # predicted = predicted.split()[0] if predicted else ""
        if predicted.lower() == true_label.lower():
            correct += 1
        total += 1

    accuracy = correct / total if total > 0 else 0.0
    print(f"Test accuracy: {accuracy:.4f}")
    return accuracy

# Run evaluation
evaluate_on_test_set(test_df)

# test_dataset = test_dataset.map(tokenize_function, batched=True)
# eval_results_after = trainer.evaluate(test_dataset)